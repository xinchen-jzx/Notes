# MLServing

**Contextual Sparsity**ï¼š
- ICML23 - Deja Vu: Contextual Sparsity for Efficient LLMs at Inference 
- ACL24 - LLM in a flash: Efficient Large Language Model Inference with Limited Memory
- SOSP24 - PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU