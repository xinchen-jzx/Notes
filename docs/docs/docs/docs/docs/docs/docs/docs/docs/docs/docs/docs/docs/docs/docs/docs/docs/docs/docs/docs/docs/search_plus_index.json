{"./":{"url":"./","title":"Introduction","keywords":"","body":"Hi there 👋 🔭 My name is Jane, an undergraduate student from the National University of Defense Technology. 👀 My research interest includes Managed Runtime, Compiler, MLSys, and Large Language Models (LLM). 📫 How to reach me: zexinjian@gmail.com "},"MLSys/GPUManagement/":{"url":"MLSys/GPUManagement/","title":"GPU Management","keywords":"","body":"GPU Management "},"MLSys/MLServing/":{"url":"MLSys/MLServing/","title":"MLServing","keywords":"","body":"MLServing SOSP24 - PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU "},"MLSys/MLServing/sosp24-PowerInfer.html":{"url":"MLSys/MLServing/sosp24-PowerInfer.html","title":"SOSP24 - PowerInfer","keywords":"","body":"sosp24-PowerInfer PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU 1. 背景 大语言模型因其智力水平越来越高而备受关注，特别是在代码生成、AI Agent等领域得到越来越多的应用。然而，LLMs由于其较大的参数量目前主要部署在配备有高端专用GPU服务器的数据中心中。另一方面，由于数据隐私、降低推理成本等需求的推动，在本地平台上部署LLMs的趋势正在兴起，特别是在配备消费级GPU的个人电脑上。 2. 问题 在消费级GPU上部署LLMs面临着巨大挑战，这主要是由于其庞大的显存需求。当模型参数对显存的需求超出GPU显存时，现有的参数卸载方法都会引入显著的速度下降问题。本文认为LLM推理中内存问题的关键原因是硬件架构与LLM推理特性之间的局部性不匹配。当前的硬件架构设计了一个针对数据局部性优化的内存层次结构。理想情况下，一个小的、频繁访问的工作集应该存储在GPU中。相比之下，更大的、访问频率较低的数据更适合存储在CPU中。然而，每次LLM推理迭代都需要访问整个参数集，其总大小太大而无法装入单个GPU，因此没有表现出任何局部性，从而阻碍了高效的局部性利用。 3. 观察 LLM中的神经元激活遵循幂律分布：一小部分神经元在各种输入中持续贡献了大部分激活 (超过80%，热激活)，而大部分神经元参与剩余的激活，这些激活是在运行时根据输入确定的 (冷激活)。这一观察表明LLMs在高激活稀疏性中存在固有的激活局部性，可以用来解决前面提到的局部性不匹配问题。 4. 设计 基于这种局部性观察，本文推出PowerInfer。其核心理念是通过将少量热神经元分配给GPU，而将构成大多数的冷神经元由CPU管理，来利用LLM推理中的局部性。具体来说，PowerInfer通过两步过程来利用LLM推理中的局部性： 在离线阶段，PowerInfer基于神经元的统计激活频率预先分类出热神经元和冷神经元，并将它们分别预加载到GPU和CPU上。 在运行时，它使用在线预测器来识别对于每个特定输入可能被激活的神经元。这种方法允许GPU和CPU独立处理它们各自的激活的神经元，从而最小化PCIe数据传输。 5. 测试 实验结果表明，PowerInfer能够实现平均7.23倍的加速比，最高达11.69倍的加速。这主要取决于PowerInfer能够让超过80%的计算由GPU来承担，显著地缓解了局部性不匹配的问题。 6. Q&A Q1: 这种模型的稀疏性需要finetune模型吗？ A1: 有些模型天然存在这样的稀疏性，例如OPT，NVIDIA推出的Minitron；有些模型则需要进行finetune以激发其稀疏性。 Q2: 热神经元是只有一层有的还是所有层都存在？ A2: 神经元的冷热性在整个模型中存在，并不局限于一层。 "},"LLM/":{"url":"LLM/","title":"LLM","keywords":"","body":"LLM NeurIPS17 - Attention is All You Need "}}