{"./":{"url":"./","title":"Introduction","keywords":"","body":"Hi there 👋 🔭 My name is Jane, an undergraduate student from the National University of Defense Technology. 👀 My research interest includes Managed Runtime, Compiler, MLSys, and Large Language Models (LLM). 📫 How to reach me: zexinjian@gmail.com 📽️ Movies、Readings "},"MLSys/GPUManagement/":{"url":"MLSys/GPUManagement/","title":"GPU Management","keywords":"","body":"GPU Management "},"MLSys/MLServing/":{"url":"MLSys/MLServing/","title":"MLServing","keywords":"","body":"MLServing Contextual Sparsity： ICML23 - Deja Vu: Contextual Sparsity for Efficient LLMs at Inference ACL24 - LLM in a flash: Efficient Large Language Model Inference with Limited Memory SOSP24 - PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU "},"MLSys/MLServing/sosp23-vLLM.html":{"url":"MLSys/MLServing/sosp23-vLLM.html","title":"SOSP23 - vLLM","keywords":"","body":"SOSP23 - vLLM Efficient Memory Management for Large Language Model Serving with PagedAttention "},"MLSys/MLServing/icml23-DejaVu.html":{"url":"MLSys/MLServing/icml23-DejaVu.html","title":"ICML23 - DejaVu","keywords":"","body":"ICML23 - DejaVu Deja Vu: Contextual Sparsity for Efficient LLMs at Inference Time ICML23中比较有意思的一篇文章。它的核心方法是在推理阶段，利用当前输入动态地选择部分网络参数来进行推理，而不是使用全参。所以可以将其看作是一种\"动态剪枝\"的方法。苹果最近推出的LLM in flash算是对该方法的直接应用，可见DejaVu未来在移动端部署LLM的潜力会非常巨大。更重要的是，方案很简单。 提高LLM的推理速度在算力极为有限的情况下具有非常重要的现实意义。剪枝作为一种重要的模型轻量化手段，它的核心假设是模型参数的稀疏性：可以剪掉某些不重要的参数来减少模型参数量，并尽可能维持模型的表现。 在实际使用中，通常使用特定的剪枝算法从一个较大的LLM出发 (假设为模型A)，通过剪枝得到一个较小的LLM (假设为模型B)。在推理阶段，使用模型B完成计算，这个简单过程如下图所示。 在推理阶段，对于任何输入X来说，LLM B的产生过程与它无关。所以可以把这种剪枝方法称为\"静态剪枝\"，而静态剪枝基于的稀疏性假设称为\"静态稀疏性\"。 在DejaVu中，LLM B与输入X相关。所以可以把这种剪枝方法称为\"动态剪枝\"，而它基于的稀疏性假设称为\"动态稀疏性\"。在原论文中，使用的词是Contextual Sparsity。Contextual强调的是LLM B的产生与输入X有关，如下图所示。 上图中，LLM B的产生过程受输入X的影响。DejaVu要解决的核心是：如何基于输入X从预训练好的LLM A中快速产生LLM B。 1. Contextual Sparsity Contextual Sparsity的本质是强调对于一个预训练好的LLM A，它的weight的重要性依赖于输入X。例如对于两个不同的输入X1和X2，经动态剪枝得到的LLM B是不一样的。这就是Contextual的含义，它指LLM B依赖于上下文，而上下文就是模型的输入。 相对于非上下文稀疏性或静态稀疏性，上下文稀疏性在效率和准确度之间有更好的权衡，这句话怎么理解呢？ 如上图所示，如果直接做静态稀疏和非上下文稀疏 (主要是指静态剪枝和随机动态剪枝)，会使准确度快速下降，而上下文稀疏化则会好很多。原因是上下文稀疏化不仅取决于单个输入标记，还取决于它们之间的相互关联。上图中的Theoretical Reduction代表几倍稀疏化。 但是Contextual Sparsity本身在LLM中存在吗？ 原文验证Contextual Sparsity的方法很简单。首先，使用输入X进行一次前向推理，并记录输出具有较大norm的MHA (Multi-Head Attention)中的head和MLP中的神经元。 具体实现起来也比较容易。例如对于MHA，它的每一个head的输出是一个矩阵，我们只需要对这个输出矩阵计算它的L2 Norm，然后从所有head中挑选出L2 Norm最大的那几个head即可，如下图所示。 上图示例中，输入X的序列长度N=10，维度d=6。假设MHA的head数量为3，那么MHA的输出对应于三个不同的head，图中由不同颜色表示。只需计算每个head对应输出 (不同颜色区域)的L2 Norm，然后找出较大的head即可。 对于MLP，有一点小区别。对某个特定的token，MLP的输出是一个向量。但此时不能计算整个向量的L2 Norm，而是要看哪一个输出维度的L2 Norm较大。这是因为整个向量是依靠所有神经元计算得到的，而每一个维度对应于一个神经元，如下图所示。 MLP是逐position执行的，所以它的物理意义应该按照图中红色框的形式来理解：每一列是输入中每一列 (token)的变换。但是在寻找有效神经元时，需要按照行来挑选。换言之，需要计算每一行的L2 Norm，然后挑出具有较大Norm的神经元。 在找到具有较大Norm的head和神经元后，原论文中使用相同的输入X再一次进行前向推理，但此时仅使用挑选出的部分head和MLP中的部分神经元参与计算。原文中发现，仅使用挑选出的部分head和神经元，几乎不影响模型的效果。 也就是说，通过这样一个简单的实验验证，作者们认为LLM中存在Contextual Sparsity：LLM中存在与输入强相关的部分高效参数。仅使用这些部分参数，能得到与使用全参模型几乎一致的表现。 再强调一次，不同输入X对应的高效参数部分是不一致的。这区别于传统剪枝方法，这也是为什么称为Contextual Sparsity的原因。 注：Transformer论文中MHA的每个head输出是通过拼接成一个与输入尺寸一致的tensor，然后再接全连层做变换。论文中因为需要挑选部分head，所以MHA的流程有一点点小变化：每个head的输出会先接一个全连层来变化为与输入尺寸一致的tensor；然后对所有head的输出求平均和。所以不论选多少个head，MHA的输出尺寸都是一致的。 前文提到，可以通过计算Attention head和MLP中神经元输出的L2 Norm来找到\"高效参数\"。但这个比例大概是多少？例如，假设我们已经将所有的head的L2 Norm和所有MLP中神经元的L2 Norm都计算出来了，那么究竟选top多少的head和神经元作为\"高效参数\"？ 原文基于OPT模型的实验结论如下： Attention head的稀疏率约为80%； MLP中神经元的稀疏率约为95%。 即：在实际推理过程中，我们可以只使用约20%的Attention head和约5%的MLP神经元，就能达到和全参模型差不多的效果。 2. 稀疏性预测 想要利用前文提到的稀疏性来加快推理速度，需要有方法来提前准确地预测对于当前输入X，哪些head和哪些MLP中的神经元是\"高效参数\"。 这里需要两个模型来做预测。第一个模型用来预测MHA中哪些head是\"高效的\"；第二个模型用来预测MLP中哪些神经元是\"高效的\" (神经元实际上对应于参数矩阵的某一列或某一行。至于究竟是列还是行，依赖于对参数矩阵的定义形式)。 在DejaVu中，这两个模型的实现都使用了一个两层MLP。 以预测Attention的head编号为例，假设head数为256，只需要将MLP的输出层的大小设为256，并为每一个输出使用sigmoid来做一个二分类即可 (\"选择\"or\"不选择\")。 训练数据依靠一个完整的、训练好的LLM来产生。在这个LLM的推理过程中，通过记录它的Attention输入和Attention输出，并计算不同head的L2 Norm，再基于一个L2 Norm的阈值t将head分为正例和负例。 预测MLP中需要选择的神经元编号的思路与上述基本一致。 下面以一个Transformer模块为例，来说明一种朴素的实现方法。 假设当前的Transformer模块为网络的第lll层，它对应的两个稀疏性预测模型记为SPAl\\text{SP}^{l}_{A}SP​A​l​​和SPMl\\text{SP}^{l}_{M}SP​M​l​​。SP\\text{SP}SP是Sparsity Predictor的缩写；下标AAA表示Attention；下标MMM表示MLP；上标lll表示当前的网络层数。 符号SPAl\\text{SP}^{l}_{A}SP​A​l​​和SPMl\\text{SP}^{l}_{M}SP​M​l​​意味着不同网络层使用的稀疏性预测模型是不同的。 假设当前的输入为xxx，首先使用模型SPAl\\text{SP}^{l}_{A}SP​A​l​​预测需要选择的head编号： a=SPAl(x)a=\\text{SP}_{A}^{l}(x)a=SP​A​l​​(x) aaa可以理解为是一个索引集合，表示选中的head编号。 out1=MHAa(x)out_1=\\text{MHA}_a(x)out​1​​=MHA​a​​(x) MHA\\text{MHA}MHA的下标aaa表示MHA只使用了aaa中编号所对应的head。 b=SPMl(out1)b=\\text{SP}_{M}^{l}(out_1)b=SP​M​l​​(out​1​​) bbb可以理解为是一个索引集合，表示选中的神经元编号。 out2=MLPb(out1)out_2=\\text{MLP}_b(out_1)out​2​​=MLP​b​​(out​1​​) 3. 高效实现 基于MLP的稀疏性预测必须要尽可能高效地实现，否则整个LLM的推理时间可能会因为引入了额外的MLP而变得更慢。 3.1 并行化稀疏预测 假设xxx为当前层lll的输入。并行化的方案是直接使用xxx来预测l+1l+1l+1层的head编号和MLP神经元编号。这相当于是提前预测。这样做的好处是：对第lll层来说，它可以在计算MHA和MLP的同时预测下一层的head编号和MLP神经元编号。 因此原本只能串行执行的四步操作，其中预测编号的两步可以和剩下的两步并行执行了。 但是为什么能这样做？原文给出的理由是：Slowly Changing Embedding across Layers。因为缓慢，所以提前一步去预测似乎也比较合理。 上图中的左图是连续的两个网络层token embedding之间的余弦相似度。上图中的右图是间隔n层的token embedding之间的余弦相似度。两个图非常直观，LLM中token的embedding的变化确实非常缓慢。因此，使用前一层的输入去提前预测下一层的两个编号，也就比较合理。 3.2 Kernel Fusion 在PyTorch中实现文中的稀疏矩阵乘法需要先用预测的编号索引去从参数矩阵中取对应的参数，这会导致3次I/O： 读参数矩阵W； 取完对应索引后写参数矩阵W1； 读W1进行矩阵乘法计算。 显然对于当下的场景，2、3是多于的两步。但在PyTorch提供的算子中，只能这样去执行。 所以很简单的策略是单独写一个kernel，将1、2、3步合并在一起，这样I/O只有1次。 "},"MLSys/MLServing/acl24-LLM_in_a_flash.html":{"url":"MLSys/MLServing/acl24-LLM_in_a_flash.html","title":"ACL24 - LLM in a flash","keywords":"","body":"ACL24 - LLM in a flash LLM in a flash: Efficient Large Language Model Inference with Limited Memory 1. Flash Memory and DRAM 在移动端设备中 (如手机)，DRAM可理解为\"运行时内存\"，Flash Memory可理解为\"存储空间\"。做一个简单的类比，在PC中，DRAM对应于内存；Flash Memory对应于硬盘存储 (注意：仅仅是对应于，实现方案并不一样)。 在通常的LLM推理阶段，LLM都是直接加载到DRAM中的。一个7B半精度LLM，完全加载进DRAM所需的存储空间超过14GB。考虑到目前主流手机的DRAM最高也就16GB的水平，在端侧直接使用DRAM来加载7B LLM面临巨大挑战。 下图给出了一个移动端标准的存储结构示意图。 Flash Memory的特点是大存储、低带宽；而DRAM的特点是小存储，高带宽。 现在的问题是：模型大小 > DRAM，所以无法将模型全部加载进DRAM。 苹果的解决方案是将LLM放在Flash Memory中，在每次需要进行推理时，仅仅将部分必要参数加载到DRAM中。 苹果的整个方案重点解决两个问题： 如何快速识别出哪些模型参数是必要的； 考虑到由Flash memory到DRAM的带宽较低，如何加快由Flash memory到DRAM的传输效率。 论文中从三个不同方面做了尝试。 2. 减少数据传输量 2.1 方法1：Selective Persistence Strategy 对于常见的LLM而言，它的模型参数主要由Attention参数和MLP参数两部分构成，其中Attention参数占比约为1/3，MLP参数占比约为2/3。除此，还有参数量级可忽略不计的Embedding层的参数。 因为Attention参数量相对较少，所以苹果的方案是将Attention参数和Embedding层的参数直接加载到DRAM中。 这就是所谓的Selective Persistence Strategy，其意为：有选择性地把部分参数常驻在DRAM中。而这部分常驻的参数就是Attention参数和Embedding参数。原因是因为它们占比较小。 2.2 方法2：Anticipating ReLU Sparsity 这里主要借鉴了DejaVu的思路：MLP层的输出只有不到10%的值是激活状态，一般把这种现象称为稀疏性。稀疏性越强，则非激活状态的值就越多。 要注意，此处的稀疏性一般称为\"Contextual Sparsity\"。即MLP层的哪些神经元会激活，与当前的输入相关。 苹果照搬了DejaVu的方法，使用一个两层MLP来预测哪些神经元会激活。方法也很简单，假设神经元个数为4096，只需要将MLP的输出层的大小设为4096，并为每一个输出使用sigmoid来做一个二分类即可 (\"选择\"or\"不选择\")。 注意： 不同Transformer层使用的预测模型不同； 同一个Transformer层中的MLP一般有两层，它们的激活神经元始终保持相同。 在能够准确预测的前提下，每次在推理时动态加载预测为激活神经元对应的参数即可。 2.3 方法3：Sliding Window 根据2.2小节中介绍的稀疏性可知，在每一次LLM进行前向推理时，它都需要使用模型预测每一个MLP层中激活神经元的编号，并将所需的神经元所对应的权重由Flash memory加载到DRAM中。 因为LLM的推理阶段是逐token进行的，这意味着在生成不同token的时候，需要加载到DRAM中的MLP的参数也不同。 用一个简单的例子来说明这个基础概念，只考虑第$l$层的Transformer模块。在处理当前token x1x_1x​1​​时，该层使用模型$P^l$预测MLP会激活的神经元编号，假设为{0,1,3,5}\\lbrace 0, 1, 3, 5 \\rbrace{0,1,3,5}，并将其对应的参数Wl1W_l^1W​l​1​​从Flash memory加载到DRAM中，然后进行推理。 在处理下一个token x2x_2x​2​​时，将Wl1W_l^1W​l​1​​从DRAM中删除，再使用模型PlP^lP​l​​预测MLP会激活的神经元编号，假设为{0,2,3,6}\\lbrace 0, 2, 3, 6 \\rbrace{0,2,3,6}，并将其对应的参数Wl2W_l^2W​l​2​​从Flash memory加载到DRAM中，然后进行推理。 注意到在我们的例子中，两次前向推理时，第lll层的Transformer结构中MLP有部分被预测为激活的神经元是重叠的：{0,3}\\lbrace 0, 3 \\rbrace{0,3}。所以实际上在进行第二次前向推理时，没有必要把Wl1W_l^1W​l​1​​完全从DRAM中删除，而是将其中编号为{1,5}\\lbrace 1, 5 \\rbrace{1,5}神经元对应的参数删除，再将编号为{2,6}\\lbrace 2, 6 \\rbrace{2,6}的神经元对应的参数读入即可。这样可以减少I/O的总开销。 这就是Sliding Window的核心思想：保留处理过去kkk个token时的激活神经元所对应的参数在DRAM中，并在处理当前token时只对： 部分多余的参数进行删除； 缺少的参数进行加载。 Sliding Window的核心假设是LLM在处理相邻token时产生的稀疏性具有相似性。 3. 提高传输吞吐量 3.1 Bundling Columns and Rows 通常LLM中的MLP层包含两个全连层。在忽略激活函数的情况下，这两个全连层可以写为： y=xW1W2Ty=xW_1W_2^Ty=xW​1​​W​2​T​​，其中输入x∈R1×d,W1,W2∈dRd×1dx \\in R^{1 \\times d},W_1,W_2 \\in dR^{d \\times 1d}x∈R​1×d​​,W​1​​,W​2​​∈dR​d×1d​​，ddd是维度。 在2.2小节中提到，稀疏性预测是对MLP中两个全连层同时进行的。也就是说，如果我们预测结果是第一个全连层中0号神经元不会被激活，那么该预测结果同样也适用于第二个全连层：第二个全连层的0号神经元也不会被激活。 对于第一个全连层的参数矩阵W1W_1W​1​​，第iii个神经元对应于它的第iii列；对于第二个全连层的参数矩阵W2TW_2^TW​2​T​​，第iii个神经元对应于它的第iii行。 当第iii个神经元被预测为激活时，需要同时读取W1W_1W​1​​的第iii列和W2TW_2^TW​2​T​​的第iii行。所以为了提高读取速度，可以将W1W_1W​1​​的每一列列和W2TW_2^TW​2​T​​的对应行拼接起来存储，如下图所示： 3.2 Bundling Based on Co-activation 这是一个原文尝试过，但被验证为无效的策略。 原文中猜测某些神经元之间可能存在一些紧密联系。比如对于两个神经元a和b，当a激活时，b也会激活 (或者当b激活时，a也会激活)。 因此可以通过分析来找到每个神经元的\"closest friend\" (与该神经元同时激活频率最高的其它某个神经元)。然后在存储Flash memory中存储时，也将它们的参数拼接存在一起。这样的读取效率更高。 但该方法之所以无效，主要原因是可能会存在某个神经元i，它是其它很多神经元的\"closest friend\"。这样导致的问题则是神经元i被额外传输了太多次，导致实际的I/O成本增加了。 4. Optimized Data Management in DRAM 虽然DRAM的数据读取速度比Flash memory快很多，但当需要对其中数据进行大量、高频读写时，它的时耗仍然不可忽略。在本文介绍的内容中，对DRAM的读写主要发生在对MLP层中所需神经元对应参数的删除与新增。 为此，论文中设计了一种特殊的数据结构来对DRAM中的数据做精细化管理。该数据结构的核心变量如下： Matrix：按照\"Bundling Columns and Rows\"的方法存储激活神经元所对应的参数； bias：激活神经元所对应的bias参数； num_used：激活神经元的个数； last_k_active：过去k个token所对应的激活神经元编号； Pointer：当前行参数对应的神经元编号。 以上图中最左图为例。图中矩阵的每一行是特定神经元 (Pointer对应的编号)的两个全连层参数的拼接向量。对于网络中的每一个Transformer层中的MLP，会预先分配一个大小为Ri×2dR_i \\times 2dR​i​​×2d的矩阵。RiR_iR​i​​表示预分配的行数，它表示预估的激活神经元最大值，靠实验得到。ddd是全连层的维度；2d2d2d是因为拼接了两个全连层的参数。 通过预分配一个足够大的空间，可以避免因反复分配而导致的额外开销。 该矩阵中的行对应的是当前存储在DRAM中激活神经元的参数。前文提到，当处理新的token时，需要将不会被激活的神经元删除，并添加新的会被激活的神经元。所以最重要的两个操作是\"删除\"和\"新增\"。 当需要删除某个神经元时，只需将num_rows的数量减1，并将最后一行Copy至被删除行，结果如图4的中图所示。虚线框表示当前有效的数据； 当需要\"新增\"时，直接将其对应的参数由Flash memory中copy至该矩阵中即可，无需额外分配存储空间。 5. 实验结果 苹果这篇paper的主要关注点在于：让LLM在运行时内存受限的情况下能高效地跑起来。所以论文的实验主要对比了各种情况下I/O导致的时耗，如下图所示。 "},"MLSys/MLServing/sosp24-PowerInfer.html":{"url":"MLSys/MLServing/sosp24-PowerInfer.html","title":"SOSP24 - PowerInfer","keywords":"","body":"SOSP24 - PowerInfer PowerInfer: Fast Large Language Model Serving with a Consumer-grade GPU 1. 背景 大语言模型因其智力水平越来越高而备受关注，特别是在代码生成、AI Agent等领域得到越来越多的应用。然而，LLMs由于其较大的参数量目前主要部署在配备有高端专用GPU服务器的数据中心中。另一方面，由于数据隐私、降低推理成本等需求的推动，在本地平台上部署LLMs的趋势正在兴起，特别是在配备消费级GPU的个人电脑上。 2. 问题 在消费级GPU上部署LLMs面临着巨大挑战，这主要是由于其庞大的显存需求。当模型参数对显存的需求超出GPU显存时，现有的参数卸载方法都会引入显著的速度下降问题。本文认为LLM推理中内存问题的关键原因是硬件架构与LLM推理特性之间的局部性不匹配。当前的硬件架构设计了一个针对数据局部性优化的内存层次结构。理想情况下，一个小的、频繁访问的工作集应该存储在GPU中。相比之下，更大的、访问频率较低的数据更适合存储在CPU中。然而，每次LLM推理迭代都需要访问整个参数集，其总大小太大而无法装入单个GPU，因此没有表现出任何局部性，从而阻碍了高效的局部性利用。 3. 观察 LLM中的神经元激活遵循幂律分布：一小部分神经元在各种输入中持续贡献了大部分激活 (超过80%，热激活)，而大部分神经元参与剩余的激活，这些激活是在运行时根据输入确定的 (冷激活)。这一观察表明LLMs在高激活稀疏性中存在固有的激活局部性，可以用来解决前面提到的局部性不匹配问题。 4. 设计 基于这种局部性观察，本文推出PowerInfer。其核心理念是通过将少量热神经元分配给GPU，而将构成大多数的冷神经元由CPU管理，来利用LLM推理中的局部性。具体来说，PowerInfer通过两步过程来利用LLM推理中的局部性： 在离线阶段，PowerInfer基于神经元的统计激活频率预先分类出热神经元和冷神经元，并将它们分别预加载到GPU和CPU上。 在运行时，它使用在线预测器来识别对于每个特定输入可能被激活的神经元。这种方法允许GPU和CPU独立处理它们各自的激活的神经元，从而最小化PCIe数据传输。 5. 测试 实验结果表明，PowerInfer能够实现平均7.23倍的加速比，最高达11.69倍的加速。这主要取决于PowerInfer能够让超过80%的计算由GPU来承担，显著地缓解了局部性不匹配的问题。 6. Q&A Q1: 这种模型的稀疏性需要finetune模型吗？ A1: 有些模型天然存在这样的稀疏性，例如OPT，NVIDIA推出的Minitron；有些模型则需要进行finetune以激发其稀疏性。 Q2: 热神经元是只有一层有的还是所有层都存在？ A2: 神经元的冷热性在整个模型中存在，并不局限于一层。 "},"LLM/":{"url":"LLM/","title":"LLM","keywords":"","body":"LLM LLM Foundations NeurIPS17 - Attention is All You Need "},"LLM/LLMModels.html":{"url":"LLM/LLMModels.html","title":"LLM Foundations","keywords":"","body":"1. 什么是Base/Chat/Instruct/4Bit模型 大模型库中的Base、Chat、Instruct和4Bit通常指的是不同类型或配置的预训练语言模型。它们的区别主要在于训练目标、用途和模型参数的精度。 Base模型 (base) 定义：Base模型通常是指未经特定任务微调的基础预训练模型，在训练过程中最初被开发和优化的，它旨在平衡性能和资源消耗； 用途：这些模型通常用于进一步的微调，以适应特定任务或应用场景。如：智能对话、文本内容生成等； 特点：它们包含了大量通用知识，但没有针对特定任务进行优化。 Chat模型 (chat) 定义：Chat模型专门为对话系统 (聊天机器人)设计和优化； 用途：用于生成自然语言对话，能够理解上下文并生成连贯且有意义的回复。如：聊天机器人、智能助力； 特点：通常经过大量对话数据微调，具备更好的上下文理解能力和对话生成能力。 Instruct模型 (instruct) 定义：Instruct模型是为遵循指令或完成特定任务而设计和优化的模型； 用途：用于执行具体指令，如回答问题、生成文本、翻译等任务； 特点：经过指令数据集微调，能够更好地理解和执行用户提供的指令。 4-Bit模型 (4bit) 定义：4-Bit模型使用低精度 (4位)进行量化，以减少内存占用和计算资源需求； 用途：适用于资源受限的环境，如移动设备或嵌入式系统，同时保持较高的性能表现； 特点：通过量化技术显著减少了模型大小和计算复杂度，但可能会牺牲部分精度。 "},"LLM/MLServing.html":{"url":"LLM/MLServing.html","title":"ML Serving","keywords":"","body":"LLM Serving 1. LLM特点 1.1 自回归模型简介 LLM最核心的模块是自回归的Transformer模型。自回归模型可通过序列的联合条件概率公式表示： P(x)=P(x1)⋅P(x2∣x1)⋅...⋅P(xn∣x1,...,xn−1) P(x)=P(x_1) \\cdot P(x_2|x_1) \\cdot ... \\cdot P(x_n|x_1,...,x_{n-1}) P(x)=P(x​1​​)⋅P(x​2​​∣x​1​​)⋅...⋅P(x​n​​∣x​1​​,...,x​n−1​​) 如公式所示，自回归模型生成文本时需要计算多个时刻的条件概率。在实际模型运行时，自回归模型生成文本需要经过多轮迭代生成完整文本。生成文本的流程可以抽象为生成token的循环，循环内部主要分为以下两个步骤： 生成下一个token。每次迭代需要根据模型的文本输入 (Prompt)以及已生成的tokens上文信息，计算出下一个token的生成概率，即(xn∣x1,...,xn−1)(x_n|x_1,...,x_{n-1})(x​n​​∣x​1​​,...,x​n−1​​)，最后通过采样或者搜索策略找出本次迭代生成的token。 判断是否结束生成循环。当生成一个结束符标志 (EOF token)，表示模型已经完成该请求，此时跳出循环。另外，受制于GPU显存大小、模型长文本生成效果等因素，一般结束条件还包括最大生成长度 (max_dec_length)。 所以，基于自回归模型的LLM生成文本过程是一种基于迭代的串行操作，是一种内存受限 (memory-bound)操作 (计算访存比较低，模型权重访问频繁)，其性能瓶颈主要在访存上。这样使得GPU的计算能力没法得到充分利用，从而影响LLM服务的吞吐。 1.2 KV Cache的来源 自回归Transformer通过SelfAttention机制建模。SelfAttention流程主要可分为三个步骤： 将输入的序列xxx映射到三个向量空间中，分别得到三个向量q,k,vq,k,vq,k,v。 计算q,kq,kq,k向量注意力得分，得到注意力得分aija_{ij}a​ij​​，表示第iii个位置的token和第jjj个位置 (j≤ij \\le ij≤i)的token的得分。个注意力得分组成一个向量aia_ia​i​​，表示第iii个位置的token和其余位置iii之前所有位置token的得分向量。 注意力得分向量aia_ia​i​​与vvv做内积，得到SelfAttention的最终输出oio_io​i​​。 qi=Wqxi,ki=Wkxi,vi=Wvxi q_i=W_qx_i,k_i=W_kx_i,v_i=W_vx_i q​i​​=W​q​​x​i​​,k​i​​=W​k​​x​i​​,v​i​​=W​v​​x​i​​ aij=exp(qiTkj/d)∑t=1iexp(qiTkt/d),oi=∑j=1iaijvk a_{ij}=\\frac{\\text{exp}(q_i^Tk_j/\\sqrt{d})}{\\sum_{t=1}^{i}\\text{exp}(q_i^Tk_t/\\sqrt{d})},o_i=\\sum_{j=1}^ia_{ij}v_k a​ij​​=​∑​t=1​i​​exp(q​i​T​​k​t​​/√​d​​​)​​exp(q​i​T​​k​j​​/√​d​​​)​​,o​i​​=​j=1​∑​i​​a​ij​​v​k​​ 通过上述公式可知，对于自回归Transformer来说，在计算aija_{ij}a​ij​​时，除了需要计算得到的qi,ki,viq_i,k_i,v_iq​i​​,k​i​​,v​i​​，还需要kj,vj(ji)k_j,v_j(jk​j​​,v​j​​(ji)。由于之前迭代生成第j个token时已经计算过kj,vj(ji)k_j,v_j(jk​j​​,v​j​​(ji)，并且后续迭代生成可以直接复用kj,vjk_j,v_jk​j​​,v​j​​，所以自回归的Transformer模型都使用缓存的方式将计算过的k,vk,vk,v向量缓存到显存中，避免每次迭代重复计算上文k,vk,vk,v向量。这就是KV Cache。 2. LLM服务 吞吐是LLM服务最重要的性能指标之一，提升LLM服务的吞吐可以降低服务部署成本。提升模型服务的吞吐最直观的方式是将尽量多的请求合并成一个模型输入 (即batching)，使单次推理能完成尽量多的请求计算。因为对一个合并请求单次推理可以共享模型权重，多个请求推理只需访问一次权重，计算访存比提升，访存开销能被batch请求所增加的计算开销覆盖，所以单次推理合并越多请求，吞吐越高。对于模型服务来说，单次推理最大合并请求数主要受显存制约。所以，提升模型服务吞吐这个研究话题可以转变成在显存一定的情况下，提升单次推理最大合并请求数。这也是LLM服务最关注的问题。 2.1 LLM服务WorkFlow 在LLM服务中，用户一个请求包含一个输入的prompt tokens(x1,...,xn)(x_1,...,x_n)(x​1​​,...,x​n​​)，即一段文本，服务生成TTT个tokens(xn+1,...,xn+T)(x_{n+1},...,x_{n+T})(x​n+1​​,...,x​n+T​​)，并将tokens合并成一段文本返回客户端。如上文自回归模型的介绍，LLM 服务需要逐token生成。由于KVCache主要用于加速生成token时的attention计算，而生成第一个token时并不存在KVCache，无法直接使用prompt的KVCache进行attention计算，所以LLM服务生成文本的流程主要分为两个阶段：Prompt阶段 (生成第一个token，在TGI里也称Prefill阶段)以及Decode阶段 (生成第二及后续token)。 2.1.1 Prompt阶段 2.1.2 Decode阶段 2.2 Batching策略 如上文提到，Batching能提升模型服务的吞吐。常规的模型服务的Batching策略如下： 设置一个Batching延时以及最大Batch Size； 收到新请求时，将请求入队，并判断请求队列中的请求数是否达到最大Batch Size或者最早请求与最迟请求的到达时间差是否达到Batching延时，如果条件满足，则执行第3步骤，否则继续休眠直至Batching超时或者新请求到达； 对合并后的模型输入进行模型推理，得到完整的模型结果，将结果返回客户端，服务返回步骤2的状态，等待下一个Batch的请求。 对于生成式的LLM模型服务来说，使用常规的模型服务Batching策略会有两个问题： 请求端到端延时提升。论文里提到两个延时：Batching延时以及请求在队列里的等待延时。在高并发场景下，请求在队列的等待耗时约等于前序批次请求的推理耗时总和。由于大模型是自回归模型，推理耗时主要在Decode阶段。Decode阶段的耗时与生成的tokens数 (迭代次数)成正比。生成文本长度越长，Decode耗时越长。生成单个token耗时在毫秒级别，那么生成千级别token数Decode耗时基本在秒级别，所以等待延时可达秒级别。线上服务基本无法接受这么长的请求等待延时。 GPU计算、存储资源有效利用率低。模型输入输出的长度不一致。文本长度不一致对模型输入和输出端的影响不同： 对于模型输入而言，由于文本输入长度不同，在构造模型输入的时候需要进行padding预处理，那么在进行模型推理的时候就会引入对padding位的计算，浪费了GPU计算资源 (不过现在已经有一些变长推理优化策略解决这个问题[2])。 推理引擎需要将batch内所有请求完成文本生成后，才会对下一个batch的请求进行推理。由于不同请求生成长度可能不同，在整个batch到达某一轮迭代时，部分请求可能已经完成推理，但此时推理引擎仍然会继续完成剩余请求推理，直至所有请求完成推理。每当一轮迭代完成一个请求的推理，意味着下一轮请求多了一个空转的请求推理，而请求队列里的请求又无法直接使用当前batch内已完成推理的请求使用的GPU计算资源，导致GPU资源浪费。 之所以造成上述两个问题，是因为常规的模型服务采用Request-Level方式调度请求，如上图所示。当Batch全部请求完成推理后，再将请求队列中的请求插入到Batch输入，然后进行下一个Batch的推理。每个请求推理阶段由若干个Decode迭代组成，整个推理阶段耗时太长，并且每个请求推理耗时又不一致，导致上述两个问题的出现。 目前，[3]针对LLM服务请求调度优化提出Iteration-Level请求调度方式，与Request-Level方式相比，请求调度在时间维度上粒度减小了。具体做法是，当完成一轮Decode迭代时，如果Batch内有请求完成推理，则将该请求剔除出Batch；如果有新的请求进入，则将新请求加入到当前Batch进行推理。通过这种调度优化方式，可以同时解决上述两大问题： 延时方面：可以大幅度降低请求端到端延时。由于队列的请求在Decode阶段动态地插入到Batch，所以请求的队列等待时间从原来的秒级别下降到毫秒级别 (仅需等待一轮迭代即可插入到Batch中)。 资源利用率方面：提升GPU资源有效利用率。由于队列的请求在Decode阶段每一轮结束后动态地插入到Batch，而Batch也在迭代过程中动态地删除已完成推理的请求，这样避免了GPU空转，提升资源利用率。 所以，vLLM 将基于Iteration-Level请求调度策略之上优化模型服务。 2.3 LLM显存占用分析 LLM服务显存占用主要由三部分组成：模型权重、KVCache以及其他激活值。 对于KV Cache，现有的LLM服务会先预分配一块显存池，然后对显存池进行读写实现对KV Cache的访问。但是，为每个请求预分配一段KV Cache会造成内部碎片、外部碎片等显存浪费。vLLM正是在KV Cache的显存管理上实现了按需分配的策略，从而提升服务的最大batch size。 2.4 总结 现有LLM服务存在请求的端到端耗时高、GPU资源有效利用率低以及KV Cache显存开销大这三大问题，针对这三个问题，提升吞吐有以下几种方法： Batching策略优化：使用Iteration-level的请求调度策略替代Request-level的请求调度策略； 低精度量化：量化模型权重，提升KV Cache的显存占用上限；量化激活值，包括KV Cache，提升KV Cache最大batch size； KV Cache显存管理策略优化：使用按需分配显存策略代替预分配显存策略。 vLLM主要通过1、3两种方式来优化LLM服务吞吐，方式2与方式1、3不冲突，在vLLM之上可以综合量化方式继续优化吞吐。 "},"LLM/neurips17-Transformer.html":{"url":"LLM/neurips17-Transformer.html","title":"NeurIPS17 - Transformer","keywords":"","body":"neurips17 - Transformer Attention is All You Need 1. 引言 Transformer模型的其中一个优点，就是使得模型训练过程能够并行计算。在RNN中，每一个time step的计算都依赖于上一个time step的输出，这就使得所有的time step必须串行化，无法并行计算；而在Transformer中，所有time step的数据，都是经过Self Attention计算，使得整个运算过程可以并行化计算。如下图所示： Transformer使用了Seq2Seq任务中常用的结构 -- 包括两个部分：Encoder和Decoder，如下图所示： 2. 从整体宏观来理解Transformer 首先，我们将整个模型视为黑盒。在机器翻译任务中，接收一种语言的句子作为输入，然后将其翻译成其他语言输出。 中间部分的Transformer可以拆分为两个部分：左边是编码部分 (encoding component)，右边是解码部分 (decoding component)。 其中编码部分是多层的编码器 (Encoder)组成 (Transformer的论文中使用了6层编码器，这里的层数6并不是固定的，也可以根据实验效果来修改层数)。同理，解码部分也是由多层的解码器 (Decoder)组成 (论文里也使用了6层的解码器)。 每一个编码器在结构上都是一样的，但它们的权重参数是不同的。每一个编码器里面，可以分为2层： Self-Attention Layer Feed Forward Neural Network (前馈神经网络，缩写为FFNN) 输入编码器的文本数据，首先会经过一个Self Attention层，这个层在处理一个词的时候，不仅会使用这个词本身的信息，也会使用句子中其他词的信息。接下来，Self Attention层的输出会经过前馈神经网络。同理，解码器也有这两层，但是这两层中间还插入了一个Encoder-Decoder Attention层，这个层能帮助解码器聚焦于输入句子的相关部分。 3. 从细节来理解Transformer 3.1 Transformer的输入 和通常的NLP任务一样，我们首先会使用词嵌入算法 (embedding algorithm)，将每个词转换为一个词向量。实际中向量一般是256或者512维。为了简化起见，这里将每个词的转换为一个4维的词向量。 那么整个输入的句子是一个向量列表，其中有3个词向量。在实际中，每个句子的长度不一样，我们会取一个适当的值，作为向量列表的长度。如果一个句子达不到这个长度，那么就填充全为0的词向量；如果句子超出这个长度，则做截断。句子长度是一个超参数，通常是训练集中的句子的最大长度。 编码器 (Encoder)接收的输入都是一个向量列表，输出也是大小同样的向量列表，然后接着输入下一个编码器。 第一个编码器的输入是词向量，而后面的编码器的输入是上一个编码器的输出。 这里我们可以注意到Transformer的一个重要特性：每个位置的词向量经过编码器都有自己单独的路径。具体来说，在Self Attention层中，这些路径之间是有依赖关系的；而在Feed Forward (前馈神经网络)层中，这些路径之间是没有依赖关系的。因此这些词向量在经过Feed Forward层中可以并行计算。 3.2 Encoder (编码器) 上面我们提到，一个编码器接收的输入是一个向量列表，它会把向量列表输入到Self Attention层，然后经过feed-forward neural network (前馈神经网络)层，最后得到输出，传入下一个编码器。 每个位置的词都经过Self Attention层，得到的每个输出向量都单独经过前馈神经网络层，每个向量经过的前馈神经网络都是一样的。 4. Self-Attention整体理解 举一个例子，假设我们想要翻译的句子是：The animal didn't cross the street because it was too tired. 这个句子中的it是一个指代词，那么it指的是什么呢？它是指animal还是street？这个问题对人来说，是很简单的，但是对算法来说并不是那么容易。当模型在处理 (翻译)it的时候，Self Attention机制能够让模型把it和animal关联起来。 同理，当模型处理句子中的每个词时，Self Attention机制使得模型不仅能够关注这个位置的词，而且能够关注句子中其他位置的词，作为辅助线索，进而可以更好地编码当前位置的词。 相反：RNN在处理一个词时，会考虑前面传过来的hidden state，而hidden state就包含了前面的词的信息。而Transformer使用Self Attention机制，会把其他单词的理解融入处理当前的单词。如下图： 5. Self-Attention的细节 5.1 计算Query向量、Key向量、Value向量 下面我们先看下如何使用向量来计算Self Attention，然后再看下如何使用矩阵来实现Self Attention (矩阵运算的方式，使得Self Attention的计算能够并行化，这也是Self Attention最终的实现方式)。 计算Self Attention的第1步是：对输入编码器的每个词向量，都创建3个向量，分别是：Query向量，Key向量，Value向量。这3个向量是词向量分别和3个矩阵相乘得到的，而这个矩阵是我们要学习的参数。 注意，这3个新得到的向量一般比原来的词向量的长度更小。 5.2 计算Attention Score (注意力分数) 第2步，是计算Attention Score (注意力分数)。假设我们现在计算第一个词Thinking的Attention Score (注意力分数)，需要根据Thinking这个词，对句子中的其他每个词都计算一个分数。这些分数决定了我们在编码Thinking这个词时，需要对句子中其他位置的每个词放置多少的注意力。 这些分数，是通过计算Thinking对应的Query向量和其他位置的每个词的Key向量的点积而得到的。如果我们计算句子中第一个位置单词的Attention Score (注意力分数)，那么第一个分数就是q1和k1的内积，第二个分数就是q1和k2的点积。 第3步就是把每个分数除以某个数，这是为了在反向传播时，求取梯度更加稳定。 第4步，接着把这些分数经过一个Softmax层，Softmax可以将分数归一化，这样使得分数都是正数并且加起来等于1。 这些分数决定了在编码当前位置的词时，对所有位置的词分别有多少的注意力。很明显，在上图的例子中，当前位置的词会有最高的分数，但有时，关注到其他位置上相关的词也很有用。 第5步，得到每个位置的分数后，将每个分数分别与每个Value向量相乘。这种做法背后的直觉理解就是：对于分数高的位置，相乘后的值就越大，我们把更多的注意力放到了它们身上；对于分数低的位置，相乘后的值就越小，这些位置的词可能是相关性不大的，这样我们就忽略了这些位置的词。 第6步是把上一步得到的向量相加，就得到了Self Attention层在这个位置的输出。 上面这张图，包含了Self Attention的全过程，最终得到的当前位置的向量会输入到前馈神经网络。但这样每次只能计算一个位置的输出向量，在实际的代码实现中，Self Attention 的计算过程是使用矩阵来实现的，这样可以加速计算，一次就得到所有位置的输出向量。 6. 使用矩阵计算Self-Attention 第一步是计算Query，Key，Value的矩阵。首先，我们把所有词向量放到一个矩阵X中，然后分别和3个权重矩阵相乘，得到Q，K，V矩阵。 接着，由于我们使用了矩阵来计算，我们可以把上面的第2步到第6步压缩为一步，直接得到Self Attention的输出。 7. 多头注意力机制 Transformer的论文通过增加多头注意力机制 (一组注意力称为一个attention head)，进一步完善了Self Attention层。这种机制从如下两个方面增强了attention层的能力： 它扩展了模型关注不同位置的能力。在上面的例子中，第一个位置的输出z1包含了句子中其他每个位置的很小一部分信息，但z1可能主要是由第一个位置的信息决定的。当我们翻译句子：The animal didn't cross the street because it was too tired时，我们想让机器知道其中的it指代的是什么。这时，多头注意力机制会有帮助。 多头注意力机制赋予attention层多个\"子表示空间\"。下面我们会看到，多头注意力机制会有多组的权重矩阵 (在Transformer的论文中，使用了8组注意力 (attention heads))。每一组注意力的的权重矩阵都是随机初始化的。经过训练之后，每一组注意力可以看作是把输入的向量映射到一个\"子表示空间\"。 接着，我们把每组K、Q、V计算得到每组的Z矩阵，就得到8个Z矩阵。 接下来就有点麻烦了，因为前馈神经网络层接收的是1个矩阵 (其中每行的向量表示一个词)，而不是8个矩阵。所以我们需要一种方法，把8个矩阵整合为一个矩阵。 怎么才能做到呢？我们把矩阵拼接起来，然后和另一个权重矩阵相乘。 8. 使用位置编码来表示序列的顺序 到目前为止，我们阐述的模型中缺失了一个东西，那就是表示序列中单词顺序的方法。 为了解决这个问题，Transformer模型对每个输入的向量都添加了一个向量。这些向量遵循模型学习到的特定模式，有助于确定每个单词的位置，或者句子中不同单词之间的距离。这种做法背后的直觉是：将这些表示位置的向量添加到词向量中，得到了新的向量，这些新向量映射到Q/K/V，然后计算点积得到attention时，可以提供有意义的信息。 9. 残差连接 编码器结构中有一个需要注意的细节是：编码器的每个子层 (Self Attention层和FFNN)都有一个残差连接和层标准化 (layer-normalization)。 在解码器的子层里面也有层标准化 (layer-normalization)。 10. Decoder (解码器) 编码器一般有多层，第一个编码器的输入是一个序列，最后一个编码器输出是一组注意力向量K和V。这些注意力向量将会输入到每个解码器的Encoder-Decoder Attention层，这有助于解码器把注意力集中输入序列的合适位置。 在完成了编码 (encoding)阶段之后，我们开始解码 (decoding)阶段。解码 (decoding)阶段的每一个时间步都输出一个翻译后的单词。 接下来会重复这个过程，直到输出一个结束符，Transformer就完成了所有的输出。每一步的输出都会在下一个时间步输入到下面的第一个解码器。Decoder就像Encoder那样，从下往上一层一层地输出结果。正对如编码器的输入所做的处理，我们把解码器的输入向量，也加上位置编码向量，来指示每个词的位置。 解码器中的Self Attention层，和编码器中的Self Attention层不太一样：在解码器里，Self Attention层只允许关注到输出序列中早于当前位置之前的单词。具体做法是：在Self Attention分数经过Softmax层之前，屏蔽当前位置之后的那些位置。 Decoder Attention层的原理和多头注意力 (MultiHeaded Self Attention)机制类似，不同之处是：Decoder Attention层是使用前一层的输出来构造Query矩阵，而Key矩阵和Value矩阵来自于编码器 (Encoder)最终的输出。 11. 最后的线性层和Softmax层 Decoder最终的输出是一个向量，其中每个元素是浮点数。我们怎么把这个向量转换为单词呢？这是由Softmax层后面的线性层来完成的。 线性层就是一个普通的全连接神经网络，可以把解码器输出的向量，映射到一个更长的向量，这个向量称为logits向量。现在假设我们的模型有10000个英语单词 (模型的输出词汇表)，这些单词是从训练集中学到的。因此logits向量有10000个数字，每个数表示一个单词的分数。我们就是这样去理解线性层的输出。 然后，Softmax层会把这些分数转换为概率 (把所有的分数转换为正数，并且加起来等于1)。然后选择最高概率的那个数字对应的词，就是这个时间步的输出单词。 "},"Lover/":{"url":"Lover/","title":"My Lover","keywords":"","body":" 我愿意在有限的时间里无限的爱着你。 /* 头像大小调整 */ .avatar { width: 80px; /* 头像宽度 */ height: 80px; /* 头像高度 */ vertical-align: -20px; border-radius: 50%; margin-right: 5px; margin-bottom: 5px; -webkit-box-shadow: 1px 1px 1px rgba(0,0,0,.1), 1px 1px 1px rgba(0,0,0,0.1), 1px 1px 1px rgba(0,0,0,0.1); box-shadow: 1px 1px 1px rgba(0,0,0,.1), 1px 1px 1px rgba(0,0,0,0.1), 1px 1px 1px rgba(0,0,0,0.1); border: 2px solid #fff; } function timing() { // 开始时间 - 格式: 年-月-日 时:分:秒 let start = '2021-9-3 00:00:00' let startTime = new Date(start).getTime() let currentTime = new Date().getTime() let difference = currentTime - startTime let m = Math.floor(difference / (1000)) let mm = m % 60 // 秒 let f = Math.floor(m / 60) let ff = f % 60 // 分钟 let s = Math.floor(f / 60) // 小时 let ss = s % 24 let day = Math.floor(s / 24 ) // 天数 return \" \" + day + \" day \" + ss + \" h \" + ff + \" m \" + mm +' s' } setInterval(()=>{ document.getElementById('Timing').innerHTML = timing() }, 1000) --> "},"life/":{"url":"life/","title":"Year-end Summary","keywords":"","body":""},"life/2024-summary.html":{"url":"life/2024-summary.html","title":"2024 Summary","keywords":"","body":"2024，路漫漫其修远兮 路漫漫其修远兮，吾将上下求索。 曾子曰“吾日三省吾生，为人谋而不忠乎？与朋友交而不信乎？传不习乎？”2024年去到了很多地方，看到了外面世界的广阔，也遇到了很多优秀的人，这段时间内也发现了自己很多不足，希望自己可以一直坚持自己的理想。须知少时凌云志，曾许人间第一流。 ⬅️ 回顾2024 天地生君子，君子理天地。 写年终总结之前，感慨万千，但真到要写的时候，却不知道如何下笔，那就随便聊聊吧。 学习 对于我而言2024年的关键词就是保研了，因为2024年下半年就开始保研推免了，所以今年大部分的时间都是在为了保研做准备 -- 疯狂刷绩点、打比赛、联系老师实习等等，不过最后还是去到了自己满意的课题组啦！在这过程中我很幸运，遇到了很多志同道合的朋友，和他们一起合作了大大小小的项目，一起熬夜，一起为比赛中性能的提升而欢呼雀跃，当然也取得了相对而言不错的成绩，但也留下了遗憾。但是我自己觉得，也正是因为有了遗憾，所以就一直激励着我们下次要更加努力、思考更加周到，从而获得自我进步。 比赛 2024年年初的数模美赛，当时和华仔、瓜哥一直准备这个比赛，读美赛优秀论文，每天汇报一下自己关于如何应对这次美赛的心得想法。比赛的时候和他们一直开着腾讯会议交流，过程彼此鼓励，特别是最后一天，熬了一整晚（差点g了），虽说结果不是很令人满意（呜呜呜呜），但其实我也很感谢这次经历，我们彼此之间更加默契了。贴一张当时我们视频的图，哈哈哈哈哈哈。 4月份的程序设计竞赛 -- 蓝桥杯和天梯赛，通过这几次比赛发现自己确实不适合算法这个赛道，要一直刷题刷题，可能是因为高中三年一直刷题，对刷题产生阴影了吧😭。但结果相对而言比较满意。（图好像找不到了😭） 6月份的网络挑战赛，和定中合作了一次，当时正好一顿事情堆在一个时间段，所以这个比赛当时准备的也不充分，不过也获得了不错的成绩。（这个也没拍照😭） 7月份的集创赛，自己对于硬件不是很了解，所以当时准备这个比赛的时候有很多需要学的硬件知识，不过很感谢爽子和焯哥，大佬带我，哈哈哈哈哈哈，取得了相对不错的成绩。（当时好像也忘记拍照了😭） 8月份的编译竞赛，这个比赛我们小组准备了挺长时间，从今年3月份一直到8月份，但是临到比赛截止提交时却接连出问题，最后结果也有点差强人意，不过通过这个比赛对系统也产生了浓厚的兴趣，小组成员之间也越来越默契，当然也希望我们明年可以弥补这个遗憾。（不过有一说一，杭州那边真是美食荒漠，好难吃😭）。贴个图。 今年最后一个比赛就是11月份的红山操作系统了，开始下定决心学习Rust这门语言，Rust第X次入门😂。这次和牛仔也算合作了一次，当然明年会继续和牛仔合作，希望在明年的全国操作系统比赛中拿到更好的名次。贴个图。 论文 今年印象最深刻的还有一个就是发表论文，全程参与了论文的全过程，论文ddl之前一直和师哥、老师讨论实验结果的合理性、论文结构的润色等等，对于我而言是一次比较全新的体验，很开心可以和田老师合作，这次我学习到了很多知识。最后我们所做的工作也被Nips录取了，哈哈哈哈哈哈，开心。贴个图。 生活 读万卷书，行万里路。 2024年借着夏令营、比赛等诸多的机会，去到了很多地方，看到了很多没看到的风景，遇到了很多有趣的人，很幸运。 聚散 首先是今年和家的见面明显多了起来，哈哈哈哈，一起去西安旅游，爬了华山，看了城墙、大雁塔、大唐不夜城、兵马俑等等等，吃了肉夹馍、油泼面、biangbiang面、岐山臊子面等等等；在北京一起玩耍，逛了清华、北大，看了颐和园；畅谈人生理想......之前和和家一直是打电话，畅谈人生理想，彼此鼓励，很高兴遇到了家，和他成为了人生知己，哈哈哈哈哈哈，我相信我们俩慢慢的都会实现自己的梦想，成为越来越好的人。 今年年初和朱同学一起去到了桂林那边旅游玩耍，当时其实也是临时起意（哈哈哈哈，来了一场说走就走的旅行），所以准备的就比较仓促，不过还是看到了很多壮阔的风景。 展望2025 我将继续寻找，就算这无尽的星辰令我的寻找希望渺茫，就算我将单枪匹马。 在2025年，我希望可以做一些事情，如此年轻的我，很想要去改变世界。同时也希望自己可以一直保持着自己的热爱。须知少时凌云志，曾许人间第一流。 "},"Reading/":{"url":"Reading/","title":"Reading","keywords":"","body":""},"Reading/My_Altay.html":{"url":"Reading/My_Altay.html","title":"我的阿勒泰","keywords":"","body":"我的阿勒泰 我们想要赚更多的钱，过更好一些的生活。但是要想赚更多的钱的话，得先到更偏远的地方，过更糟糕一点的生活。其实再想一想，那些更糟糕的生活同以后可能会有的更好的生活放到一起平摊了，折算下来的话，其实还是一日一日不好不坏的生活。 最安静与最孤独的成长，也是能使人踏实、自信、强大、善良的。 呜呼！如果养马只是为了吃肉，生活该索然无味到什么地步？ 但是，哪怕到了现在，拥有一匹马 -- 这仍然是多么巨大的愿望啊！至于被一匹马高高载着，风驰电掣地奔向远方 -- 那情景让人一想到便忍不住心血来潮。 觉得自己是在拿小库兰\"打掩护\"......觉得自己永远是一个\"独自\"的人，唉，有些时候，没有爱情真是丢人。 以后，我会爱上别的人的，年轻岁月如此漫长...... 在这样美丽着的世界里，一个人的话总是令人难过的。所以我就有所渴望了，所以麦西拉就出现了...... 但我想，造成野生动物的濒临灭亡，其实并不是仅仅因为猎人的缘故吧？这人世间更多的欲望远比猎人的狩猎行为更为黑暗贪婪，且更为狂妄。 "},"Friends/":{"url":"Friends/","title":"My Friends","keywords":"","body":"My Friends "}}