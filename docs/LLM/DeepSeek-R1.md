# DeepSeek-R1

DeepSeek-R1验证了三个思路：
1. DeepSeek-R1-Zero：**纯RL**，展现了自我进化能力，但存在可读性问题；
2. DeepSeek-R1：**冷启动 + 多阶段训练**，在多个推理benchmark上取得了和OpenAI-o1-1217相近的水平；
3. 小模型蒸馏：知识蒸馏后的小模型在推理benchmark上也表现出强大的竞争力。

## 1. 背景和动机

- **研究问题**：如何通过强化学习（RL）有效提升大型语言模型（LLM）的推理能力？
- **问题背景**：
    - 近年来，LLM在各个领域都取得了显著进展，但推理能力仍有提升空间。
    - 之前的研究大多依赖于大量的监督式微调（SFT）数据，但获取高质量的SFT数据成本高昂。
    - OpenAI的o1系列模型通过增加思维链（Chain-of-Thought，CoT）推理过程的长度来提升推理能力，但如何有效进行测试时（test-time）扩展仍是开放问题。
    - 一些研究尝试使用基于过程的奖励模型、强化学习和搜索算法来解决推理问题，但没有达到OpenAI的o1系列模型的通用推理性能水平。
- **论文动机**：探索是否可以通过纯强化学习来让LLM自主发展推理能力，而无需依赖监督式数据。

## 2. 相关研究

- **监督式微调（SFT）**：之前的研究通常依赖SFT来增强模型性能。然而，SFT需要大量标注数据，成本高且耗时。
- **推理时扩展**：OpenAI的o1系列模型通过增加CoT推理长度来实现推理能力扩展，但测试时扩展的挑战仍然存在。
- **基于过程的奖励模型**：一些研究采用过程奖励模型来引导模型进行推理。然而，这些模型在实际应用中存在局部性。
- **强化学习**：强化学习已被用于提升推理能力，但通常与SFT数据结合使用，难以探索纯RL的潜力。
- **搜索算法**：如蒙特卡洛树搜索等算法也被用于增强推理，但效果有限。

## 3. 核心思路

- **DeepSeek-R1-Zero：纯强化学习**
    - 直接在基础模型上应用强化学习，不使用任何SFT数据。
    - 探索LLM在纯RL环境下的自演化过程，使其自主发展推理能力。
- ***DeepSeek-R1：冷启动 + 多阶段训练*
    - 使用少量高质量的CoT数据进行冷启动，预热模型。
    - 进行面向推理的强化学习，提升模型在推理任务上的性能。
    - 使用拒绝采样和监督微调，进一步提升模型的综合能力。
    - 再次进行强化学习，使模型在所有场景下都表现良好。
- **知识蒸馏**
    - 将DeepSeek-R1的推理能力蒸馏到更小的模型中，使小模型也具有强大的推理能力。

## 4. 方案与技术

- **强化学习算法**：使用名为GRPD（Group Relative Policy Optimization）的方法，跳过传统RL中与策略模型等规模的critic网络